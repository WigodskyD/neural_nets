# -*- coding: utf-8 -*-
"""
Created on Wed Sep  2 02:27:01 2020

This file trains a preexisting model with new data and more epochs.

@author: dawig
"""

import os
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model
from os import getcwd
import matplotlib.pyplot as plt
from tensorflow.keras.applications.inception_v3 import preprocess_input as pre_inp


#-----------------------------------
#Parameters:
data_pathway = "D:/Tensorflow_practice_data/beech_or_lancair_classifier"
batch_size = 30
img_height = 300
img_width = 300
epochs_to_train = 30


#_________________________________________________________________

#________
# Import data with ImageDataGenerator
#________
tf.keras.backend.clear_session()
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=pre_inp,
                                                  rescale=1./255,
                                                  shear_range=0.3,
                                                  zoom_range=0.25,
                                                  horizontal_flip=True,
                                                  rotation_range=30,
                                                  validation_split=0.2)        # set validation split

train_generator = train_datagen.flow_from_directory(
    data_pathway,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary',
    subset='training') # set as training data

validation_generator = train_datagen.flow_from_directory(
    data_pathway, # same directory as training data
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary',
    subset='validation') # set as validation data


    
#________
# Compile model
#________
#lr_schedule = tf.keras.callbacks.LearningRateScheduler(
#    lambda epoch: 1e-8 * 10**(epoch / 2))
#optimizer = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)

model_to_continue = tf.keras.models.load_model('keras_model.h5')

#------to fine tune, open more layers, but slow learning rate(elsewhere)
for layer in model_to_continue.layers[:-1]:
    layer.trainable = False
for layer in model_to_continue.layers[-1:]:
    layer.trainable = True
#-------------------------
model_to_continue.compile(optimizer = optimizer, 
              loss = 'binary_crossentropy', 
              metrics = ['acc'])


model_to_continue.summary()

cp_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath = 'D:/Tensorflow_practice_data/checkpoint_fine_tune',
        period=15        
        )

#________
# Fit model
#________
history = model_to_continue.fit_generator(
        train_generator,
        validation_data = validation_generator,
        epochs = epochs_to_train,
        callbacks=[cp_callback]
        )

#plt.semilogx(history.history["lr"], history.history["loss"])
#plt.axis([1e-8, 1e-4, 0, 2])
plt.plot(history.history['acc'], color = 'coral')
plt.plot(history.history['loss'], color = 'red')
plt.plot(history.history['val_acc'], color = 'turquoise')
plt.plot(history.history['val_loss'], color = 'cadetblue')


